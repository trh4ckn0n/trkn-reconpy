#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import subprocess, os, shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import questionary
from rich.console import Console
from rich.progress import track
from datetime import datetime

console = Console()

# -----------------------------
# Configuration
# -----------------------------
TOOLS = {
    "nmap": "nmap",
    "httpx": "httpx",
    "nuclei": "nuclei",
    "dnsx": "dnsx",
    "alterx": "alterx",
    "dalfox": "dalfox"
}

THREADS = 10

def check_tools():
    for tool, cmd in TOOLS.items():
        if not shutil.which(cmd):
            console.print(f"[red]‚ùå {tool} n'est pas install√© ou pas dans le PATH[/red]")
            exit(1)

def run_cmd(cmd, capture=False):
    """Ex√©cute une commande shell"""
    console.print(f"[cyan]üíª Running:[/cyan] {cmd}")
    if capture:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout.strip()
    else:
        subprocess.run(cmd, shell=True)

def create_results_dir(target):
    path = Path(f"results_{target}")
    path.mkdir(exist_ok=True)
    return path

# -----------------------------
# Fonctions principales
# -----------------------------
def discover_subdomains(target, results_dir):
    console.print(f"[yellow]üåê D√©couverte des sous-domaines pour {target}...[/yellow]")
    perm_file = results_dir / "permutations.txt"
    resolved_file = results_dir / "resolved.txt"
    
    run_cmd(f"echo {target} | alterx generate -o {perm_file} --silent")
    run_cmd(f"dnsx -l {perm_file} -resp-only > {resolved_file}")
    
    count = sum(1 for _ in open(resolved_file))
    console.print(f"[green]‚úÖ {count} sous-domaines r√©solus[/green]")
    return resolved_file

def scan_http(resolved_file, results_dir):
    console.print("[yellow]üîó Analyse HTTP des h√¥tes actifs...[/yellow]")
    http_targets = results_dir / "http_targets.txt"
    
    cmd = f"httpx -l {resolved_file} --follow-redirects --no-verify --timeout 10"
    
    # Multi-threaded
    urls = open(resolved_file).read().splitlines()
    with ThreadPoolExecutor(max_workers=THREADS) as executor:
        results = list(executor.map(lambda u: run_cmd(f"httpx -u {u} --follow-redirects --no-verify --timeout 10", capture=True), urls))
    
    with open(http_targets, "w") as f:
        for r in results:
            if r: f.write(r + "\n")
    
    count = sum(1 for _ in open(http_targets))
    console.print(f"[green]‚úÖ {count} h√¥tes HTTP actifs d√©tect√©s[/green]")
    return http_targets

def scan_vulns(http_targets, results_dir):
    console.print("[yellow]‚ö° Scan de vuln√©rabilit√©s (Nuclei)...[/yellow]")
    nuclei_file = results_dir / "nuclei_results.txt"
    run_cmd(f"nuclei -l {http_targets} -t ~/nuclei-templates/ -o {nuclei_file}")
    
    console.print("[yellow]‚ö° Scan XSS avec Dalfox...[/yellow]")
    xss_file = results_dir / "dalfox_results.txt"
    run_cmd(f"dalfox file {http_targets} -o {xss_file}")
    
    console.print(f"[green]‚úÖ Scans de vuln√©rabilit√©s termin√©s[/green]")
    return nuclei_file, xss_file

def scan_nmap(resolved_file, results_dir):
    console.print("[yellow]üõ°Ô∏è Scan Nmap avec scripts NSE...[/yellow]")
    nmap_file = results_dir / "nmap_results.txt"
    
    hosts = open(resolved_file).read().splitlines()
    with ThreadPoolExecutor(max_workers=THREADS) as executor:
        results = list(executor.map(lambda h: run_cmd(f"nmap -sV --script vuln {h}", capture=True), hosts))
    
    with open(nmap_file, "w") as f:
        for r in results:
            if r: f.write(r + "\n\n")
    
    console.print(f"[green]‚úÖ Scan Nmap termin√©[/green]")
    return nmap_file

def scrape_files(http_targets, results_dir):
    console.print("[yellow]üóÇÔ∏è Scraping fichiers sensibles...[/yellow]")
    common_paths = ["admin", "login", "backup.zip", ".env", "config.php"]
    found_file = results_dir / "found_files.txt"
    
    urls = open(http_targets).read().splitlines()
    with open(found_file, "w") as f:
        for url in track(urls, description="Scanning paths..."):
            for path in common_paths:
                result = run_cmd(f"httpx -u {url}/{path} ", capture=True)
                if result:
                    f.write(result + "\n")
    console.print(f"[green]‚úÖ Scraping termin√©[/green]")
    return found_file

def generate_html_report(results_dir):
    console.print("[yellow]üìÑ G√©n√©ration du rapport HTML...[/yellow]")
    html_file = results_dir / "report.html"
    with open(html_file, "w") as f:
        f.write("<html><head><title>Recon Report</title></head><body>")
        f.write(f"<h1>Recon Report - {datetime.now()}</h1>")
        for file in results_dir.iterdir():
            if file.suffix in [".txt"]:
                f.write(f"<h2>{file.name}</h2><pre>{open(file).read()}</pre>")
        f.write("</body></html>")
    console.print(f"[green]‚úÖ Rapport HTML g√©n√©r√©: {html_file}[/green]")

# -----------------------------
# Main
# -----------------------------
def main():
    target = questionary.text("üîç Entrez le domaine cible:").ask()
    if not target:
        console.print("[red]‚ùå Aucun domaine saisi ![/red]")
        return

    results_dir = create_results_dir(target)
    
    resolved_file = discover_subdomains(target, results_dir)
    http_targets = scan_http(resolved_file, results_dir)
    nuclei_file, xss_file = scan_vulns(http_targets, results_dir)
    nmap_file = scan_nmap(resolved_file, results_dir)
    found_file = scrape_files(http_targets, results_dir)
    generate_html_report(results_dir)

    console.print(f"[cyan]üìÇ Tous les r√©sultats sont dans : {results_dir}[/cyan]")

if __name__ == "__main__":
    check_tools()
    main()
